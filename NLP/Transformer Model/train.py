# -*- coding: utf-8 -*-
"""Transformer from Scratch Model training

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15E5bgP4cBjn-4T_4VuLaSy9wtbpSvTpt
"""

import torch
import torch.nn as nn
from datasets import load_dataset
from tokenizers import Tokenizer
from tokenizers.models import WordLevel
from tokenizers.trainers import WordLevelTrainer
from tokenizers.pre_tokenizers import Whitespace
from pathlib import Path
from torch.utils.data import Dataset,DataLoader,random_split

!pip install datasets
!pip install tokenizers

def get_all_sentences(ds,lang):
  for item in ds:
    yield item['translation'][lang]
def get_or_build_tokenizer(config,ds,lang):
  #config['tokenizer_file]
  tokenizer_path=Path(config['tokenizer_file'].format(lang))
  if not Path.exists(tokenizer_path):
    tokenizer=Tokenizer(WordLevel(unk_token="[UNK]"))
    tokenizer.pre_tokenixer=Whitespace()
    trainer=WordLevelTrainer(special_tokens=["[UNK]","[PAD]","[SOS]","[EOS]"],min_frequency=2)
    tokenizer.train_from_iterator(get_all_sentences(ds,lang),trainer=trainer)
    tokenizer.save(str(tokenizer_path))
  else:
    tokenizer=Tokenizer.from_file(str(tokenizer_path))
  return tokenizer

class BilingualDataset(Dataset):
  def __init__(self,ds,tokenizer_src,tokenizer_tgt,src_lang,tgt_lang,seq_len):
    super().__init__()
    self.tokenizer_src=tokenizer_src
    self.tokenizer_tgt=tokenizer_tgt
    self.src_lang=src_lang
    self.tgt_lang=tgt_lang
    self.sos_token=torch.Tensor([tokenizer_src.token_to_id(['[SOS]'])],dtype=torch.int64)
    self.eos_token=torch.Tensor([tokenizer_src.token_to_id(['[EOS]'])],dtype=torch.int64)
    self.pad_token=torch.Tensor([tokenizer_src.token_to_id(['[PAD]'])],dtype=torch.int64)

  def __len__(self):
    return len(self.ds)

  def __getitem__(self,index):
    src_target_pair=self.ds[index]
    src_text=src_target_pair['translation'][self.src_lang]
    tgt_text=src_target_pair['translation'][self.tgt_lang]

    #tokenize the text
    enc_input_tokens=self.tokenizer_src.encode(src_text).ids
    dec_input_tokens=self.tokenizer_tgt.encode(tgt_text).ids

    #Ensures all sequence in batch have same length
    enc_num_padding_tokens=self.seq_len-len(enc_input_tokens)-2
    dec_num_padding_tokens=self.seq_len-len(dec_input_tokens)-1

    #add sos,eos,pad tokens
    if enc_num_padding_tokens<0 or dec_num_padding_tokens<0:
      raise ValueError('Sentence is too long')

    encoder_input=torch.cat(
        self.sos_token,
        torch.tensor(enc_input_tokens,dtype=torch.int64),
        self.eos_token,
        self.pad_token*enc_num_padding_tokens,
        dtype=torch.int64
    )

    decoder_input=torch.cat(
        self.sos_token,
        torch.tensor(dec_input_tokens,dtype=torch.int64),
        self.eos_token,
        self.pad_token*dec_num_padding_tokens,
        dtype=torch.int64
    )

    label=torch.cat(
        [
            torch.tensor(dec_input_tokens,dtype=torch.int64),
            self.eos_token,
            torhc.tensor([self.pad_token]*dec_num_padding_tokens,dtype=torch.int64)
        ]

    )

    assert encoder_input.size(0)==self.seq_len
    assert decoder_input.size(0)==self.seq_len
    assert label.size(0)==self.seq_len

    return {
        'encoder_input':encoder_input, #(Seq_len)
        'decoder_input':decoder_input,
        #Boolean array
        'encoder_mask':(encoder_input!=self.pad_token).unsqueeze(0).unsqueeze(0).int(), #(1,1,Seq_len)
        'decoder_mask':(decoder_input!=self.pad_token).unsqueeze(0).int() & causal_mask(decoder_input.size(0)), #(1,Seq_len)
        'label':label,
        'src_text':src_text,
        'tgt_text':tgt_text
    }
def causal_mask(size):
  #Values above the diagnol and everything else becomes 0
  mask=torch.triu(torch.ones((1,size,size)),diagonal=1).type(torch.int)
  return mask==0  #Everything that is 0 will become true

def get_ds(config):
  ds_raw=load_dataset('opus_books',f'{config["lang_src"]}--{config["lang_tgt"]}',split='train')

  #Build tokenizer
  tokenizer_src=get_or_build_tokenizer(config,ds_raw,config["lang_src"])
  tokenizer_tgt=get_or_build_tokenizer(config,ds_raw,config["lang_tgt"])

  #train test split 90 10
  train_ds_size=int(0.9*len(ds))
  val_ds_size=int(ds_raw)-train_ds_size
  train_ds_raw,val_ds_raw=random_split(ds_raw,[train_ds_size,val_ds_size])
  train_ds=BilingualDataset(train_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])
  val_ds=BilingualDataset(val_ds_raw,tokenizer_src,tokenizer_tgt,config['lang_src'],config['lang_tgt'],config['seq_len'])

  max_len_src=0
  max_len_tgt=0
  for item in ds_raw:
    src_ids=tokenizer_src.encode(item['translation'][config['lang_src']]).ids
    tgt_ids=tokenizer_tgt.encode(item['translation'][config['lang_tgt']]).ids
    max_len_src=max(max_len_src,len(src_ids))
    max_len_tgt=max(max_len_tgt,len(tgt_ids))
  print(f'Max length of source sentence:{max_len_src}')
  print(f'Max length of target sentence':{max_len_tgt}')

  train_dataloader=DataLoader(train_ds,batch_size=config['batch_size'],shuffle=True)
  val_dataloader=DataLoader(val_ds,batch_size=1,shuffle=True)
  return train_ds_raw,val_ds_raw,tokenizer_src,tokenizer_tgt

#Neccessary imports
def get_model(config,vocab_src,vocab_tgt_len):
  model=build_transformer(vocab_src,vocab_tgt_len,config['seq_len'],config['seq_len'],config['d_model'])
  return model

